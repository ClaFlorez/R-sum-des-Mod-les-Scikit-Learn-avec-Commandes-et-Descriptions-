## ğŸŒŸ RÃ©sumÃ© des ModÃ¨les Scikit-Learn avec Commandes et Descriptions (FR)

---

### ğŸ“˜ 1. Classification

PrÃ©dit une **catÃ©gorie** ou une **Ã©tiquette** (ex : maladie / pas maladie).

**ModÃ¨les principaux :**

* `LogisticRegression` : ModÃ¨le simple et efficace pour les donnÃ©es linÃ©airement sÃ©parables.

  * **Exemple d'utilisation :** PrÃ©dire si un email est "spam" ou "non-spam".
  * **RÃ©sultat attendu :** Une Ã©tiquette binaire (0 ou 1) pour chaque email.

* `RandomForestClassifier` : ForÃªt d'arbres de dÃ©cision pour une prÃ©cision robuste.

  * **Exemple :** Classification d'images (ex. chiffres manuscrits).
  * **RÃ©sultat attendu :** Classe prÃ©dite parmi plusieurs (0-9 pour MNIST).

* `KNeighborsClassifier` : Classe selon les voisins les plus proches.

  * **Exemple :** PrÃ©dire le genre d'un film basÃ© sur ses caractÃ©ristiques.
  * **RÃ©sultat :** CatÃ©gorie majoritaire parmi les k plus proches voisins.

* `SVC` : Machine Ã  vecteurs de support (kernel linÃ©aire ou non).

  * **Exemple :** SÃ©parer deux espÃ¨ces de fleurs.
  * **RÃ©sultat :** FrontiÃ¨re de dÃ©cision maximale entre classes.

* `MLPClassifier` : Perceptron multicouche, rÃ©seau de neurones de base.

  * **Exemple :** Reconnaissance de chiffres manuscrits.
  * **RÃ©sultat :** PrÃ©diction de la classe avec activation non linÃ©aire.

**Exemple de code :**

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

### ğŸ“‘ 2. RÃ©gression

PrÃ©dit une **valeur continue** (ex : revenu, prix d'une maison).

**ModÃ¨les principaux :**

* `LinearRegression` : ModÃ¨le linÃ©aire simple.

  * **Exemple :** PrÃ©dire le prix d'une maison selon sa taille.
  * **RÃ©sultat :** Une valeur numÃ©rique continue.

* `RandomForestRegressor` : Version rÃ©gression des forÃªts alÃ©atoires.

  * **Exemple :** PrÃ©dire la tempÃ©rature moyenne d'une rÃ©gion.
  * **RÃ©sultat :** Moyenne pondÃ©rÃ©e des prÃ©dictions de tous les arbres.

* `Lasso` : RÃ©gression avec rÃ©gularisation L1 (sÃ©lectionne les variables).

  * **Exemple :** PrÃ©dire le salaire avec sÃ©lection automatique des variables utiles.
  * **RÃ©sultat :** RÃ©gression linÃ©aire simplifiÃ©e avec certaines variables mises Ã  zÃ©ro.

* `SVR` : RÃ©gression par vecteurs de support.

  * **Exemple :** PrÃ©dire la demande d'Ã©nergie horaire.
  * **RÃ©sultat :** RÃ©gression avec marge d'erreur tolÃ©rÃ©e.

* `MLPRegressor` : RÃ©seau de neurones pour rÃ©gression.

  * **Exemple :** PrÃ©dire les ventes hebdomadaires d'un produit.
  * **RÃ©sultat :** Valeur continue non linÃ©aire.

**Exemple de code :**

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

### ğŸ“ƒ 3. Regroupement (Clustering)

Identifie automatiquement des **groupes** dans des donnÃ©es non Ã©tiquetÃ©es.

**ModÃ¨les principaux :**

* `KMeans` : Regroupe les points en k groupes basÃ©s sur la distance.

  * **Exemple :** Segmenter des clients selon leur comportement dâ€™achat.
  * **RÃ©sultat :** Ã‰tiquettes de cluster (0, 1, 2...)

* `DBSCAN` : DensitÃ© de points pour identifier des clusters.

  * **Exemple :** Identifier des rÃ©gions denses dans des donnÃ©es gÃ©ographiques.
  * **RÃ©sultat :** Ã‰tiquettes de cluster ou -1 pour les bruits.

* `MeanShift` : Glissement de la moyenne vers des zones de forte densitÃ©.

  * **Exemple :** DÃ©tection de motifs dans des images.
  * **RÃ©sultat :** Nombre automatique de clusters basÃ© sur la densitÃ©.

* `AgglomerativeClustering` : Approche hiÃ©rarchique ascendante.

  * **Exemple :** Grouper des documents similaires.
  * **RÃ©sultat :** Clusters hiÃ©rarchiques avec fusion progressive.

**Exemple de code :**

```python
from sklearn.cluster import KMeans
model = KMeans(n_clusters=3)
model.fit(X)
labels = model.labels_
```

---

### ğŸ“Š 4. RÃ©duction de Dimension

Simplifie les donnÃ©es en **rÃ©duisant le nombre de variables**, tout en gardant les informations importantes.

**ModÃ¨les principaux :**

* `PCA` : Analyse en composantes principales.

  * **Exemple :** Visualisation 2D de donnÃ©es Ã  100 dimensions.
  * **RÃ©sultat :** Nouvelles variables principales expliquant la variance.

* `TruncatedSVD` : SVD pour donnÃ©es clairsemÃ©es (sparse).

  * **Exemple :** RÃ©duction de matrices TF-IDF de documents.
  * **RÃ©sultat :** Projection dans un espace de dimension rÃ©duite.

* `TSNE` : RÃ©duction non linÃ©aire pour visualisation.

  * **Exemple :** Visualisation 2D de clusters.
  * **RÃ©sultat :** Carte 2D montrant la proximitÃ© locale entre points.

**Exemple de code :**

```python
from sklearn.decomposition import PCA
model = PCA(n_components=2)
X_reduced = model.fit_transform(X)
```

---

### ğŸ“„ 5. MÃ©thodes d'Ensemble

Combine plusieurs modÃ¨les simples pour **renforcer la prÃ©cision**.

**ModÃ¨les principaux :**

* `BaggingClassifier` : Moyenne de plusieurs modÃ¨les sur diffÃ©rents sous-ensembles.

  * **Exemple :** Classification de clients avec instabilitÃ©s (petits datasets).
  * **RÃ©sultat :** Meilleure stabilitÃ© et moins dâ€™overfitting.

* `AdaBoostClassifier` : Apprentissage adaptatif avec des poids.

  * **Exemple :** Classification binaire avec exemples difficiles pondÃ©rÃ©s.
  * **RÃ©sultat :** Focus sur les erreurs des modÃ¨les prÃ©cÃ©dents.

* `GradientBoostingClassifier` : Boosting par gradient (puissant mais lent).

  * **Exemple :** PrÃ©diction de dÃ©fauts de crÃ©dit.
  * **RÃ©sultat :** PrÃ©dictions amÃ©liorÃ©es par corrections successives.

* `VotingClassifier` : Vote majoritaire de plusieurs modÃ¨les.

  * **Exemple :** PrÃ©diction robuste en combinant SVM, arbre et rÃ©gression.
  * **RÃ©sultat :** Classe choisie par la majoritÃ©.

* `StackingClassifier` : Combine les sorties de plusieurs modÃ¨les via un mÃ©ta-modÃ¨le.

  * **Exemple :** Fusion de modÃ¨les faibles avec apprentissage supervisÃ©.
  * **RÃ©sultat :** ModÃ¨le final entraÃ®nÃ© sur les prÃ©dictions des autres.

**Exemple de code :**

```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)
```

---

### ğŸ”¹ Bon Ã  savoir :

Tous les modÃ¨les Scikit-Learn suivent ce schÃ©ma :

```python
model.fit(X_train, y_train)
model.predict(X_test)
model.score(X_test, y_test)  # optionnel
```

---

### ğŸ§° Structure Universelle Scikit-Learn â€“ Exemple Complet

Voici un exemple complet que l'on peut adapter Ã  presque tous les modÃ¨les (classification, rÃ©gression, etc.) :

```python
# Ã‰tape 1 : Importation
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression  # Exemple, remplaÃ§able
from sklearn.metrics import accuracy_score, classification_report

# Ã‰tape 2 : Chargement des donnÃ©es
from sklearn.datasets import load_iris
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Ã‰tape 3 : SÃ©paration train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ã‰tape 4 : Normalisation (facultatif mais recommandÃ©)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ã‰tape 5 : EntraÃ®nement du modÃ¨le
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Ã‰tape 6 : PrÃ©dictions
y_pred = model.predict(X_test_scaled)

# Ã‰tape 7 : Ã‰valuation
print("PrÃ©cision :", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=data.target_names))
```

> ğŸ’¡ Pour les modÃ¨les de rÃ©gression, utilisez `mean_squared_error`, `r2_score`, etc.
> ğŸ’¡ Pour le clustering, pas besoin de `y`, utilisez simplement `.fit(X)` puis `.labels_`

---

CrÃ©Ã© avec â¤ï¸ pour les apprenants en IA
