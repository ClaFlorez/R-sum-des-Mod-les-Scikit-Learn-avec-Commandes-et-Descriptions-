## üåü R√©sum√© des Mod√®les Scikit-Learn avec Commandes et Descriptions (FR)

---

### üìò 1. Classification

Pr√©dit une **cat√©gorie** ou une **√©tiquette** (ex : maladie / pas maladie).

**Mod√®les principaux :**

* `LogisticRegression` : Mod√®le simple et efficace pour les donn√©es lin√©airement s√©parables.

  * **Exemple d'utilisation :** Pr√©dire si un email est "spam" ou "non-spam".
  * **R√©sultat attendu :** Une √©tiquette binaire (0 ou 1) pour chaque email.

* `RandomForestClassifier` : For√™t d'arbres de d√©cision pour une pr√©cision robuste.

  * **Exemple :** Classification d'images (ex. chiffres manuscrits).
  * **R√©sultat attendu :** Classe pr√©dite parmi plusieurs (0-9 pour MNIST).

* `KNeighborsClassifier` : Classe selon les voisins les plus proches.

  * **Exemple :** Pr√©dire le genre d'un film bas√© sur ses caract√©ristiques.
  * **R√©sultat :** Cat√©gorie majoritaire parmi les k plus proches voisins.

* `SVC` : Machine √† vecteurs de support (kernel lin√©aire ou non).

  * **Exemple :** S√©parer deux esp√®ces de fleurs.
  * **R√©sultat :** Fronti√®re de d√©cision maximale entre classes.

* `MLPClassifier` : Perceptron multicouche, r√©seau de neurones de base.

  * **Exemple :** Reconnaissance de chiffres manuscrits.
  * **R√©sultat :** Pr√©diction de la classe avec activation non lin√©aire.

**Exemple de code :**

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

### üìë 2. R√©gression

Pr√©dit une **valeur continue** (ex : revenu, prix d'une maison).

**Mod√®les principaux :**

* `LinearRegression` : Mod√®le lin√©aire simple.

  * **Exemple :** Pr√©dire le prix d'une maison selon sa taille.
  * **R√©sultat :** Une valeur num√©rique continue.

* `RandomForestRegressor` : Version r√©gression des for√™ts al√©atoires.

  * **Exemple :** Pr√©dire la temp√©rature moyenne d'une r√©gion.
  * **R√©sultat :** Moyenne pond√©r√©e des pr√©dictions de tous les arbres.

* `Lasso` : R√©gression avec r√©gularisation L1 (s√©lectionne les variables).

  * **Exemple :** Pr√©dire le salaire avec s√©lection automatique des variables utiles.
  * **R√©sultat :** R√©gression lin√©aire simplifi√©e avec certaines variables mises √† z√©ro.

* `SVR` : R√©gression par vecteurs de support.

  * **Exemple :** Pr√©dire la demande d'√©nergie horaire.
  * **R√©sultat :** R√©gression avec marge d'erreur tol√©r√©e.

* `MLPRegressor` : R√©seau de neurones pour r√©gression.

  * **Exemple :** Pr√©dire les ventes hebdomadaires d'un produit.
  * **R√©sultat :** Valeur continue non lin√©aire.

**Exemple de code :**

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

### üìÉ 3. Regroupement (Clustering)

Identifie automatiquement des **groupes** dans des donn√©es non √©tiquet√©es.

**Mod√®les principaux :**

* `KMeans` : Regroupe les points en k groupes bas√©s sur la distance.

  * **Exemple :** Segmenter des clients selon leur comportement d‚Äôachat.
  * **R√©sultat :** √âtiquettes de cluster (0, 1, 2...)

* `DBSCAN` : Densit√© de points pour identifier des clusters.

  * **Exemple :** Identifier des r√©gions denses dans des donn√©es g√©ographiques.
  * **R√©sultat :** √âtiquettes de cluster ou -1 pour les bruits.

* `MeanShift` : Glissement de la moyenne vers des zones de forte densit√©.

  * **Exemple :** D√©tection de motifs dans des images.
  * **R√©sultat :** Nombre automatique de clusters bas√© sur la densit√©.

* `AgglomerativeClustering` : Approche hi√©rarchique ascendante.

  * **Exemple :** Grouper des documents similaires.
  * **R√©sultat :** Clusters hi√©rarchiques avec fusion progressive.

**Exemple de code :**

```python
from sklearn.cluster import KMeans
model = KMeans(n_clusters=3)
model.fit(X)
labels = model.labels_
```

---

### üìä 4. R√©duction de Dimension

Simplifie les donn√©es en **r√©duisant le nombre de variables**, tout en gardant les informations importantes.

**Mod√®les principaux :**

* `PCA` : Analyse en composantes principales.

  * **Exemple :** Visualisation 2D de donn√©es √† 100 dimensions.
  * **R√©sultat :** Nouvelles variables principales expliquant la variance.

* `TruncatedSVD` : SVD pour donn√©es clairsem√©es (sparse).

  * **Exemple :** R√©duction de matrices TF-IDF de documents.
  * **R√©sultat :** Projection dans un espace de dimension r√©duite.

* `TSNE` : R√©duction non lin√©aire pour visualisation.

  * **Exemple :** Visualisation 2D de clusters.
  * **R√©sultat :** Carte 2D montrant la proximit√© locale entre points.

**Exemple de code :**

```python
from sklearn.decomposition import PCA
model = PCA(n_components=2)
X_reduced = model.fit_transform(X)
```

---

### üìÑ 5. M√©thodes d'Ensemble

Combine plusieurs mod√®les simples pour **renforcer la pr√©cision**.

**Mod√®les principaux :**

* `BaggingClassifier` : Moyenne de plusieurs mod√®les sur diff√©rents sous-ensembles.

  * **Exemple :** Classification de clients avec instabilit√©s (petits datasets).
  * **R√©sultat :** Meilleure stabilit√© et moins d‚Äôoverfitting.

* `AdaBoostClassifier` : Apprentissage adaptatif avec des poids.

  * **Exemple :** Classification binaire avec exemples difficiles pond√©r√©s.
  * **R√©sultat :** Focus sur les erreurs des mod√®les pr√©c√©dents.

* `GradientBoostingClassifier` : Boosting par gradient (puissant mais lent).

  * **Exemple :** Pr√©diction de d√©fauts de cr√©dit.
  * **R√©sultat :** Pr√©dictions am√©lior√©es par corrections successives.

* `VotingClassifier` : Vote majoritaire de plusieurs mod√®les.

  * **Exemple :** Pr√©diction robuste en combinant SVM, arbre et r√©gression.
  * **R√©sultat :** Classe choisie par la majorit√©.

* `StackingClassifier` : Combine les sorties de plusieurs mod√®les via un m√©ta-mod√®le.

  * **Exemple :** Fusion de mod√®les faibles avec apprentissage supervis√©.
  * **R√©sultat :** Mod√®le final entra√Æn√© sur les pr√©dictions des autres.

**Exemple de code :**

```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)
```

---

### üîπ Bon √† savoir :

Tous les mod√®les Scikit-Learn suivent ce sch√©ma :

```python
model.fit(X_train, y_train)
model.predict(X_test)
model.score(X_test, y_test)  # optionnel
```

---

Cr√©√© avec ‚ù§Ô∏è pour les apprenants en IA
